{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine learning basics\n",
    "\n",
    "This notebook list some basic theorems, concepts and techniques used in machine learning.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Lagrange multiplier\n",
    "\n",
    "Lagrange multiplier is a technique used to solve equality-constrained optimizations of the following form:\n",
    "\n",
    "$$ \\textrm{maximize } f\\left(x\\right), $$\n",
    "\n",
    "$$ \\textrm{subject to } g\\left(x\\right) = 0. $$\n",
    "\n",
    "The solution consists of first computing the _Lagrangian_ $L$ of the problem, formally written as:\n",
    "\n",
    "$$ L = f\\left(x\\right) - \\lambda g\\left(x\\right). $$\n",
    "\n",
    "Then, take partial derivatives of $L$ with respect to the unknowns, in this case $x$ and $\\lambda$. Finaly, solve the equations:\n",
    "\n",
    "$$ \\frac{\\partial{L}}{\\partial{x}} = 0, $$\n",
    "\n",
    "$$ \\frac{\\partial{L}}{\\partial{\\lambda}} = 0. $$\n",
    "\n",
    "The intuition behind the Lagrange multiplier can be explain in the figure below.\n",
    "<img src=\"./figure/lagrange_multiplier.png\" alt=\"Lagrange-multiplier\" style=\"width: 380px;\"/>\n",
    "\n",
    "Here the blue dotted lines represent the iso-contours of $f\\left( x \\right)$. The boundary of the equality constraint $g\\left( x \\right)$ is drawn as a red curve. We know that solution exists on the red surface. So finding the solution mounts to find the position on the red line that correpsonds to the maximum of $f\\left(x\\right)$'s iso-contour value among all positions. It can be seen that, for such a position, the tangent (or normal vector) of the red curve and the iso-contour must be parallel. That is,\n",
    "\n",
    "$$ \\nabla f\\left( x \\right) = \\lambda \\nabla g\\left( x \\right). $$\n",
    "\n",
    "This corresponds to solving $\\frac{\\partial{L}}{\\partial{x}} = 0.$ The second equation $\\frac{\\partial{L}}{\\partial{\\lambda}} = 0$ boils down to the equality constraint.\n",
    "\n",
    "The Karush–Kuhn–Tucker (KKT) is a generalization of Lagrange multiplier for optimization problems with inequality constraints.\n",
    "\n",
    "#### References\n",
    "\n",
    "- Wikipedia has a detailed description with a few examples: [Lagrange](https://en.wikipedia.org/wiki/Lagrange_multiplier), [KKT](https://en.wikipedia.org/wiki/Karush%E2%80%93Kuhn%E2%80%93Tucker_conditions)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Information and entropy\n",
    "\n",
    "#### Entropy\n",
    "\n",
    "The amount of information $I$ associated to an observed event can be quantified as:\n",
    "\n",
    "$$ I = -\\log(p),$$\n",
    "\n",
    "where $p$ denotes the probablity that the observed event takes place. This intuitively makes sense: the occurence of an event with rare probablity gives an observer much more _information_ than an event that happens for sure (with probability close to 1).\n",
    "\n",
    "Entropy is defined as the average amount of information associated to a probability distribution. Let's say we have a random process that can produce $N$ discrete events with probability $p_1, \\cdots , p_n$. Then the entropy $H$ of this random process is:\n",
    "\n",
    "$$ H = - \\sum_{i = 1, \\cdots N} p_i \\log \\left( p_i \\right). $$\n",
    "\n",
    "#### Kullback-Leibler divergence\n",
    "\n",
    "The Kullback-Leibler divergence (KL-divergence) between two distributions $p$ and $q$ measures the divergence of distribution $q$ with respect to $p$. In machine learning context, $p$ can be seen as the true underlying distribution, while $q$ is an estimate of $p$. In this setting, the aim is to minimize the KL-divergence to obtain $q$ that best approximates $p$. The KL-divergence of $q$ from $p$ is mathematically written as:\n",
    "\n",
    "$$ D_{KL} \\left( p \\lVert q \\right) = - E_p [ \\log \\frac{q}{p} ] = - \\sum_i p_i \\log \\left( \\frac{q_i}{p_i} \\right). $$\n",
    "\n",
    "Two important properties of the KL-divergence are:\n",
    "\n",
    "- KL-divergence is non-negative. That is $ D_{KL} \\left( p \\lVert q \\right) \\ge 0$. This can be proved using Gibbs inequality. See below.\n",
    "- KL-divergence is non-symmetric. That is $ D_{KL} \\left( p \\lVert q \\right) \\neq D_{KL} \\left( q \\lVert p \\right) $. Therefore it is not a distance metric.\n",
    "\n",
    "#### Cross-entropy\n",
    "\n",
    "Cross entropy between two distributions $p$ and $q$ can be interpreted as the number of bits needed to encode messages coming from an underlying distribution $p$, while using another distribution $q$. The cross entropy $H(p,q)$ is expressed as:\n",
    "\n",
    "$$ H\\left( p,q \\right) = - E_p[\\log \\left( q \\right)] = - \\sum_i p_i \\log\\left( q_i \\right). $$\n",
    "\n",
    "Furthermore, the cross entropy can decomposed into the entropy of $p$ and the KL-divergence of $q$ from $p$:\n",
    "\n",
    "$$ H\\left( p,q \\right) = - E_p[\\log \\left( q \\right)] = - E_p[\\log \\left( q \\right) + \\log \\left( p \\right) - \\log \\left( p \\right)] = - E_p [ \\log \\frac{q}{p} ] - E_p [\\log \\left( p \\right)] ) =  H \\left( p \\right) + D_{KL} \\left( p \\lVert q \\right). $$\n",
    "\n",
    "In machine learning problem, the underlying distribution $p$ can be assumed to be fixed and is given as the ground truth. Minimizing the cross entropy mounts to minimize the KL-divergence of $q$ from $p$.\n",
    "\n",
    "#### Gibbs inequality\n",
    "\n",
    "Suppose $p$ and $q$ are two probability distributions. The Gibbs inequality states:\n",
    "\n",
    "$$ - \\sum_i p_i \\log \\left( p_i \\right) \\le - \\sum_i p_i \\log \\left( q_i \\right). $$\n",
    "\n",
    "The equality is obtained when $p$ and $q$ are the same distributions. Using this inequality we have:\n",
    "\n",
    "$$ - \\sum_i p_i \\log \\left( p_i \\right) + \\sum_i p_i \\log \\left( q_i \\right)  = - \\sum_i p_i \\log \\left( \\frac{p_i}{q_i} \\right) \\le 0. $$\n",
    "\n",
    "Which gives:\n",
    "\n",
    "$$ D_{KL} = - \\sum_i p_i \\log \\left( \\frac{q_i}{p_i} \\right) \\ge 0. $$\n",
    "\n",
    "An interpretation of this inequality is that, the information entropy of distribution $p$ is always less than or equal to the cross entropy between $p$ and any other distribution $q$, given that the underlying true distribution is $p$. This can be proved using the following inequality:\n",
    "\n",
    "$$ \\ln x \\le x-1, $$\n",
    "\n",
    "for any $x > 0$. The equality is obtained when x = 1. This means,\n",
    "\n",
    "$$ - \\sum_i p_i \\log \\left( p_i \\right) + \\sum_i p_i \\log \\left( q_i \\right)  = \\sum_i p_i \\log \\left( \\frac{q_i}{p_i} \\right) \\le \\sum p_i \\left( \\frac{q_i}{p_i} - 1\\right) = 0, $$\n",
    "\n",
    "which proves the Gibbs inequality.\n",
    "\n",
    "#### Jensen's inequality\n",
    "\n",
    "#### References\n",
    "\n",
    "- https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence\n",
    "- https://en.wikipedia.org/wiki/Cross_entropy#Cross-entropy_minimization\n",
    "- https://en.wikipedia.org/wiki/Gibbs%27_inequality\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Decision trees\n",
    "\n",
    "Decision trees. **C**lassification **A**nd **R**egression **T**rees (CART) is an umbrella term for decision trees. To create a decision tree from data, one of the commonly use method is the iterative partitioning algorithm such as ID3 and its variants.\n",
    "\n",
    "1. Start from the root node, compute an impurity index $I$, iterate the following.\n",
    "   - Iterate all features $k \\in \\{ 1, \\cdots, K \\}$\n",
    "   - Select feature $k$ and split the training data using this feature.\n",
    "   - Compute the impurity index $I_k$ after split.\n",
    "   - Measure the gain in purity $G = I - I_k$\n",
    "   - Select the feature that gives the maximum gain.\n",
    "\n",
    "Commonly used impurity indices are entropy and Gini's impurity.\n",
    "\n",
    "Entropy: $- \\sum_{i} p_i \\log p_i$\n",
    "\n",
    "Gini's impurity: $1 - \\sum_{i} p_i^2$\n",
    "\n",
    "In practice, the two indices have very little difference in decision tree classification / regression power. However the Gini's impurity requires less computational cost since we do not need to compute logarithm.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Boosting\n",
    "\n",
    "Boosting is an ensemble machine learning technique that computes the sum of multiple weak learner to construct a strong learner. That is:\n",
    "\n",
    "$$ f(x) = \\sum_m k_m\\left(x\\right), $$\n",
    "\n",
    "where each $k_m = \\alpha_m h\\left( x \\right)$ represents a weak learner. The term $h$ can be a classification or regression tree and $\\alpha$ correspond to this additive weight.\n",
    "\n",
    "#### AdaBoost\n",
    "\n",
    "Adaptive boosting uses sample weighting to derive each additive weak learner. \n",
    "\n",
    "Here is how to derive the AdaBoost formula for classification (ref: 1). Consider a classification problem where the prediction ouput $y \\in \\left( -1, 1 \\right)$. Define the loss function for the $m$-th iteration as the sum of the exponential loss:\n",
    "\n",
    "$$ L_{m} = \\sum_i \\exp \\left( -y_i  f_{m}\\left(x_i\\right) \\right), $$\n",
    "\n",
    "where $f_m\\left(x_i\\right) = f_{m-1} + \\alpha_i h_{m}\\left(x_i\\right)$. This can be rewritten as:\n",
    "\n",
    "$$ L_{m} = \\sum_i w_i^m\\exp \\left( -y_i  \\alpha_i h_{m}(x_i) \\right), $$\n",
    "\n",
    "with $w_i^m = \\exp \\left( -y_i  f_{m-1} \\right)$. Taking the derivative of $L^m$ with respect to $\\alpha_{m}$ yields:\n",
    "\n",
    "$$ \\alpha_{m} = \\frac{1}{2} \\ln \\frac{\\sum_{i, y_i = h_{m}\\left( x_i \\right)} w_i^m}{\\sum_{i, y_i \\neq h_{m}\\left( x_i \\right)} w_i^m}  $$\n",
    "\n",
    "The derivation detail can be found in reference 1. This yields the update formula for AdaBoost.\n",
    "\n",
    "#### Gradient Boosting\n",
    "\n",
    "In gradient boosting, the weak learners are designed to approximate the residuals. At iteration $m$, the residual is defined as:\n",
    "\n",
    "$$ r_m = y - f_{m}, $$\n",
    "\n",
    "which is precisely the gradient (derivative) of the mean squared loss $L = \\lVert y - f^{m} \\rVert ^2 $ with respect to $f_m$. The objective of the next iteration is to propose a learner $k_m = \\alpha^{m+1} h = \\arg \\min C \\left( r_m - \\alpha^{m+1} h \\right) $, where $C$ is a cost function such as mean squared error (for regression) or cross entropy (for classification). This cost is not to be confused with the loss $L$ minimized by Gradient Boosting. In practice, the minimization:\n",
    "\n",
    "$$ k_m = \\arg \\min C \\left( r_m - \\alpha^{m+1} h  \\right) $$\n",
    "\n",
    "can be solved by two steps:\n",
    "\n",
    "1. First find $\\hat{h}_m = \\arg \\min C \\left( r_m - h \\right) $.\n",
    "2. Then find $\\alpha^{m+1} = \\arg \\min L \\left( r_m - \\left( f_m +\\alpha^{m+1} \\hat{h}_m  \\right) \\right)$ using line search.\n",
    "\n",
    "Notice that, with the gradient interpretation, the sum $\\sum_{m=1}^{M} k_m\\left(x\\right)$ can indeed be seen as an iterative gradient descent process in functional space (space $\\mathcal{K}$ of weak learners $ \\{ k_m \\}_{m}$ ) from interation $1$ to $M$. Hence the name gradient in gradient boosting.\n",
    "\n",
    "The gradient term, _i.e._ the residual $r_m$ can be generalized to represent gradient of other loss functions other than squared loss. For example, the absolute difference $|y - f_{m}|$, which is more robust with respect to outliers in training data.\n",
    "\n",
    "#### XGBoost versus GBDT\n",
    "\n",
    "XGBoost is a popular implementation of (regularized) Gradient Boosting. Its particularity compared to conventional gradient boosted decision trees is as follows.\n",
    "\n",
    "- XGBoost uses regularizations. The loss function becomes\n",
    "  \n",
    "  $$ L = l(y, \\hat{y}) + \\Omega, $$ where\n",
    "\n",
    "  $$ \\Omega = \\gamma T + \\frac{1}{2} \\lVert w \\rVert^2. $$ The $T$ in the first term measures the number leaf nodes. This is similar to tree prunning, or $L_1$ regularization. The $w$ in the second term refers to the scores (output) of the leaf nodes. This is a $L_2$ regularization. \n",
    "  \n",
    "- XGBoost's loss function includes second order derivatives (Hessian). At iteration $m$, the new loss function is:\n",
    "\n",
    "  $$ L^{m} (y_i, f_m) = L^{m-1} (y_i, f_{m-1}) + \\sum_1^T \\left( G_i w_i + \\frac{1}{2} (H_i + \\lambda w_i^2) \\right) + \\gamma T,$$\n",
    "  \n",
    "  where $H$ is the Hessian matrix of the loss function with respect to input function $f_{m-1}$. By setting its gradient to 0, the optimal leaf output is derived as:\n",
    "  \n",
    "  $$ w_{i}^{*} = - \\frac{G_i}{H_i + \\lambda} $$\n",
    "\n",
    "- XGBoost use a different node split gain index. The minimum loss computed from the equation above is:\n",
    "\n",
    "  $$ L^{*} = - \\frac{1}{2} \\sum_{i=1}^{T} \\frac{G^2_i}{H_i + \\lambda} + \\gamma T. $$\n",
    "  \n",
    "  Notice that the term $\\frac{G^2_i}{H_i + \\lambda}$ measures the _contribution_ of each node to the minimum loss, and needs to be maximized. Therefore a node split gain can be defined as:\n",
    "  \n",
    "  $$ \\frac{G^2_L}{H_L + \\lambda} + \\frac{G^2_R}{H_R + \\lambda} - \\frac{G^2_L+G^2_R}{H_L+H_R + \\lambda} - \\gamma $$\n",
    "\n",
    "\n",
    "#### References\n",
    "\n",
    "1. Wikipedia contains derivation of AdaBoost formula: [link](https://en.wikipedia.org/wiki/AdaBoost)\n",
    "2. This [document](http://wepon.me/files/gbdt.pdf) explains in detail the particularity of XGBoost.\n",
    "3. This [document](http://www.ccs.neu.edu/home/vip/teach/MLcourse/4_boosting/slides/gradient_boosting.pdf) explains Gradient Boosting in a very intuitive way.\n",
    "4. This [official doc](http://xgboost.readthedocs.io/en/latest/model.html) of XGBoost explains how regularized Gradient Boosting works.\n",
    "5. More about XGBoost vs GBDT [here](https://www.zhihu.com/question/41354392).\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
