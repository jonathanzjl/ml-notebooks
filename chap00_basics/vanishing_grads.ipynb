{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Vanishing and exploding gradients\n",
    "\n",
    "#### Description\n",
    "\n",
    "The vanishing gradient problem is a phenomenon observed during the training of deep feed forward networks and recurrent neural networks (RNN).\n",
    "\n",
    "For a $K$-layer feed forward network with loss function $L$. Suppose the sigmoid function \n",
    "\n",
    "$$ \\sigma(x) = \\frac{1}{1+e^{-x}} $$\n",
    "\n",
    "is used as the activation function. During back propagation, the derivative of the loss function with respect the the weight $\\textbf{w}^{k}$ of the $k$-th layer is computed as:\n",
    "\n",
    "$$ \\frac{\\partial L}{\\partial \\textbf{w}^k} = \\frac{\\partial L}{\\partial \\textbf{a}^K} \\frac{\\partial \\textbf{a}^K}{\\partial \\textbf{a}^{K-1}} \\cdots \\frac{\\textbf{a}^{k+1}}{\\partial \\textbf{a}^{k}} \\frac{\\textbf{a}^k}{\\partial \\textbf{w}^{k}}, $$\n",
    "\n",
    "where for any layer $k \\in \\{ 1, \\cdots, K \\}$, $\\textbf{a}^k$ denotes the activation of that layer. It can written as:\n",
    "\n",
    "$$ \\textbf{a}^k = \\sigma \\left( \\textbf{w}^k \\textbf{a}^{k-1} + \\textbf{b}^k \\right), $$\n",
    "\n",
    "where $\\textbf{b}^k$ denotes the bias of the $k$-th layer.\n",
    "\n",
    "#### Solution\n",
    "\n",
    "1. One straight-forward method is clip the gradient to flatten its values that are above a certain threshold. This helps to solve the gradient exploding problem.\n",
    "\n",
    "2. LSTM introduces gating mechanism to prevent vanishing and exploding gradient problems.\n",
    "\n",
    "3. Residual networks introduce identity mapping that can aleviate vanishing gradient problems. One way to think of it is that, when the gradient flow approaches zero, the network can learn to pass its gradient through connections, which maintains a unit gradient (equal to 1).\n",
    "\n",
    "4. Different initialization schemes, such as Xavier initialization, He initialization and Batch normalization can contribute to prevent vanishing and exploding gradient problems. These techniques have the objective to regularize the activation values such that they are spreaded across the activation function's value domain, instead of concentrating around extrem values. When the later case happens, the gradient approaches zero for sigmoid and $tanh$ activations. Therefore these techniques help the gradient flow.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
