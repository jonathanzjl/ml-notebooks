{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Variational autoencoder\n",
    "\n",
    "This notebook explains the theory of **variational autoencoder** (VAE), an unsupervised generative machine learning model.\n",
    "\n",
    "- **Unsupervised** means that the input data has no _labels_. Hence VAE is usually not directly applicable for supervised tasks, such as classification or detection. Unsupervised models can be thought of as algorithms that try to understand the intrinsic properties of input data by learning a (typically low dimensional) representation of the input data.\n",
    "\n",
    "- **Generative** means that VAE can be used to generate new data that is not present in the training set.\n",
    "\n",
    "Common use cases of VAE include feature learning, dimensionality reduction, denoising, text translation and data synthesis. In this notebook, let's focus on feature learning, dimensionality reduction and data synthesis tasks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Introduction\n",
    "\n",
    "Suppose our training set is a $d \\times n$ data matrix $\\mathbf{X}$, where the $i$-th column $x^{\\left(i\\right)} \\in \\mathbb{R}^d$ with $i \\in \\{1, \\cdots, n\\}$ is a $d$-dimensional data sample. We make the hypothesis that the samples in data matrix are independent identically distributed (i.i.d).\n",
    "\n",
    "Further, let's assume that the training samples are generated from some unknown random process involving an unobserved continuous random variable $z \\in \\mathbb{R}^p$, with $p < d$. The variable $z$ is referred to as the **latent variable**. The generation process can be described as follows [1]:\n",
    "\n",
    "- First genrate $z \\sim p_{\\theta}(z)$, where $p_{\\theta}(z)$ is a **prior distribution** over the latent variable.\n",
    "\n",
    "- Then, $x$ is sampled from the **conditional distribution** $p_{\\theta}(x|z)$.\n",
    "\n",
    "This generation process can be represented as a graphical model, illustrated below.\n",
    "\n",
    "<img src=\"figure/graphical.png\" alt=\"VAE as graphical model\" style=\"width: 200px;\"/>\n",
    "<div style=\"text-align:center\"><span style=\"color:purple; font-size:1em;\">Figure 1 - probabilistic graphical model for VAE.</span></div>\n",
    "\n",
    "The solid line involves the conditional distribution $p_{\\theta}(x|z)$, also referred to as the likelihood in Bayesian statistics. The dashed line refers to the **true posterior $p_{\\theta}(z|x)$**. The **generative model** is characterized by the complete likelihood $p_{\\theta}(x,z)$, which according to Bayes rule, can be written as\n",
    "\n",
    "$$p_{\\theta}(x,z) = p_{\\theta}(x|z)p_{\\theta}(z).$$\n",
    "\n",
    "We fix the objectives as follows:\n",
    "\n",
    "1. Learn the posterior $p_{\\theta}(z|x)$, i.e. **the transformation from original sample $x$ to latent variables $z$**. This allows us to **encode** the original sample $x$ to features ($z$). When the dimensionality of $z$ is lower than that of the original input $x$, the transformation allows for a low dimensional representation of $x$.\n",
    "\n",
    "2. Learn the parameter $\\theta$ of the generative model. This allows us to **generate new data $\\hat{x}$** by drawing new samples of $z$.\n",
    "\n",
    "The following sections show how to achieve these objectives using variational Bayesian inference. When neural networks are involved, the resulting algorithm is coined in the literature as the **variational autoencoder** [1].\n",
    "\n",
    "### References\n",
    "\n",
    "- [1] [Autoencoding variational Bayes](https://ge.box.com/s/ft03d42gigjjgzzfmc0x6bjfl5gcd5c2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## General theory: autoencoding variational Bayes\n",
    "\n",
    "Consider **maximum a posteroiri** (MAP) inference for parameter $\\theta$ of the generative model. For real world data $x$, the true posterior $p_{\\theta} \\left( z|x \\right)$ is most commonly intractable. In fact, the posterior can be written as\n",
    "\n",
    "$$\n",
    "p_{\\theta} \\left( z|x \\right) = \\frac{p_{\\theta} \\left( x, z \\right)}{p_{\\theta} \\left( x \\right)} = \\frac{p_{\\theta} \\left( x, z \\right)}{ \\int p_{\\theta} \\left( x, z \\right) \\mathrm{d}z}. \\tag{1}\n",
    "$$\n",
    "\n",
    "Computing the denominator is challenging since it involves the integral over all states of $z$. One technique to tackle this problem is **variational Bayesian inference** [2], where the idea is to approximate the intractable true posterior $p_{\\theta} \\left( z|x \\right)$ by a tractable distribution $q_{\\phi}(z|x)$, such as a Gaussian distribution.\n",
    "\n",
    "To bridge the gap between $p_{\\theta} \\left( z|x \\right)$ and $q_{\\phi} \\left( z|x \\right)$, we aim to minimize the **Kullback-Leibler divergence** (KL-divergence, [3]) from $q_{\\phi}(z|x)$ to $p_{\\theta}(z|x)$, expressed as\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "D_{KL} \\left( q_{\\phi}(z|x) \\rVert p_{\\theta}(z|x) \\right) & = \\int q_{\\phi}(z|x) \\log \\frac{q_{\\phi}(z|x)}{p_{\\theta}(z|x)} \\mathrm{d}z \\\\\n",
    "& = \\int q_{\\phi}(z|x) \\left( \\log q_{\\phi}(z|x) - \\log p_{\\theta}(z|x) \\right) \\mathrm{d}z \\\\\n",
    "& = \\int q_{\\phi}(z|x) \\left( \\log q_{\\phi}(z|x) - \\log \\frac{p_{\\theta}(x,z)}{p_{\\theta}(x)} \\right) \\mathrm{d}z \\\\\n",
    "& = \\int q_{\\phi}(z|x) \\left( \\log q_{\\phi}(z|x) - \\log p_{\\theta}(x,z) \\right) \\mathrm{d}z + \\int q_{\\phi}(z|x) \\log p_{\\theta}(x) \\mathrm{d}z \\\\\n",
    "& = \\int q_{\\phi}(z|x) \\left( \\log q_{\\phi}(z|x) - \\log p_{\\theta}(x,z) \\right) \\mathrm{d}z + \\log p_{\\theta}(x) \\\\\n",
    "& = \\int q_{\\phi}(z|x) \\left( \\log q_{\\phi}(z|x) - \\log p_{\\theta}(x|z) - \\log p_{\\theta}(z) \\right) \\mathrm{d}z + \\log p_{\\theta}(x) \\\\\n",
    "& = \\int q_{\\phi}(z|x) \\log \\frac{q_{\\phi}(z|x)}{p_{\\theta}(z)} \\mathrm{d}z - \\int q_{\\phi}(z|x) \\log p_{\\theta}(x|z) \\mathrm{d}z + \\log p_{\\theta}(x) \\\\\n",
    "& = D_{KL} \\left( q_{\\phi}(z|x) \\rVert p_{\\theta}(z) \\right) - \\mathbb{E}_{q_{\\phi}(z|x)} \\left( \\log p_{\\theta}(x|z) \\right) + \\log p_{\\theta}(x). \\\\\n",
    "\\end{aligned} \n",
    "\\tag{2}\n",
    "$$\n",
    "\n",
    "Notice that $p_{\\theta}(x)$, _i.e._ the maginal distribution over $x$ is a fixed distribution. Hence minimizing $D_{KL} \\left( q_{\\phi}(z|x) \\rVert p_{\\theta}(z|x) \\right)$ is equavalent to minimizing $D_{KL} \\left( q_{\\phi}(z|x) \\rVert p_{\\theta}(z) \\right) - \\mathbb{E}_{q_{\\phi}(z|x)} \\left( \\log p_{\\theta}(x|z) \\right)$, or maximizing\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\mathcal{L}(\\theta, \\phi) = -D_{KL} \\left( q_{\\phi}(z|x) \\rVert p_{\\theta}(z) \\right) + \\mathbb{E}_{q_{\\phi}(z|x)} \\left( \\log p_{\\theta}(x|z) \\right). \\tag{3}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "Here $\\mathcal{L}(\\theta, \\phi)$ is referred to as the **variational lower-bound**. To see why it is a lower-bound, consider the marginal log-likelihood $\\log p_{\\theta}(x) $. From equation (2) and (3) we have:\n",
    "\n",
    "$$\n",
    "\\log p_{\\theta}(x) = D_{KL} \\left( q_{\\phi}(z|x) \\rVert p_{\\theta}(z|x) \\right) + \\mathcal{L}(\\theta, \\phi) \\ge \\mathcal{L}(\\theta, \\phi). \\tag{4}\n",
    "$$\n",
    "\n",
    "The inequality is obtained due to the fact that the KL-divergence is non-negative. Now let's formally write the optimization as a loss minimization problem:\n",
    "\n",
    "$$ \\textrm{minimize } \\begin{equation}\n",
    "L(\\theta, \\phi) = - \\mathbb{E}_{q_{\\phi}(z|x)} \\left( \\log p_{\\theta}(x|z) \\right) + D_{KL} \\left( q_{\\phi}(z|x) \\rVert p_{\\theta}(z) \\right). \\tag{5}\n",
    "\\end{equation}$$\n",
    "\n",
    "Now consider the graphical model in Figure 1, let's interpret the loss function $L(\\theta, \\phi)$:\n",
    "\n",
    "- The first term $- \\mathbb{E}_{q_{\\phi}(z|x)} \\left( \\log p_{\\theta}(x|z) \\right)$ is the expected negative log-likelihood of retrieving the input $x$, conditioned on the **encoded** latent variable $z$. Minimizing this term can be thought of as **minimizing the reconstruction loss**: we shall be able to reconstruct the input $x$ from its encoded representation $z$ with high probability.\n",
    "\n",
    "- The second term $D_{KL} \\left( q_{\\phi}(z|x) \\rVert p_{\\theta}(z) \\right)$ measures **the divergence from the approximate variational posterior $q_{\\phi}(z|x)$ to the prior distribution $p_{\\theta}(z)$**. \n",
    "\n",
    "  The prior distribution reflects our knowledge (or wish) about the underlying latent process and is pre-defined by us. Let's say, we fix the prior to be a multi-dimensional Gaussian distribution with zero-mean, and identify covariance matrix, _i.e._ $p_{\\theta}(z) = \\mathcal{N}(0, \\mathbf{I})$. Minimizing the second term amounts to **bringing $q_{\\phi}(z|x)$, _i.e._ the distribution of the encoded input $x$, $z$ close to $\\mathcal{N}(0, \\mathbf{I})$**. \n",
    "  \n",
    "  However, **it is not exactly a desired behavior to transform all high dimensional input samples $x$s to latent variables $z$s with distribution $\\mathcal{N}(0, \\mathbf{I})$**. Consider the case where the input data are categorized into different classes. Having a transformation that maps any input $x$ to $z \\sim \\mathcal{N}(0, \\mathbf{I})$ means that all data samples in the latent space are mixed together at the origin $o$.\n",
    "\n",
    "  The second term should be understood as a **regularization term for the latent space, on top of the reconstruction loss**. One way to think of it is that [4], **we like to avoid having latent variables scattered all around the latent space**. For instance, suppose that we have $x^{(1)}$ and $x^{(2)}$ that are both hand-written digits representing the number 2 with different writting styles. If no constraint is imposed to the latent space, there is a possibility that the encoder transforms $x^{(1)}$ and $x^{(2)}$ to latent variables $z^{(1)}$ and $z^{(2)}$ that are far away in terms of Euclidean distance. Therefore, we penalize this possibility by _**pulling**_ the latent variables towards the domain of the prior distribution $p_{\\theta}(z)$. See illustration below.\n",
    "  \n",
    "  <img src=\"figure/vae_regularization.png\" alt=\"VAE - latent regularization\" style=\"width: 400px;\"/>\n",
    "<div style=\"text-align:center\"><span style=\"color:purple; font-size:1em;\">Figure 2 - illustration of latent space regularization in VAE loss.</span></div>\n",
    "  \n",
    "  Notice that, this second term does not aim to separate data from different classes in the latent space. In other words, **VAE is not designed specifically for optimal data visualization in latent space**. For visualization task, consider methods such as t-SNE.\n",
    "\n",
    "### References\n",
    "\n",
    "- [2] [Wikipedia: variational Bayesian methods](https://en.wikipedia.org/wiki/Variational_Bayesian_methods)\n",
    "- [3] [Wkipedia: Kullback Leibler divergence](https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence)\n",
    "- [4] [Tutorial: what is variational autoencoder?](https://jaan.io/what-is-variational-autoencoder-vae-tutorial/)\n",
    "- [5] [t-SNE: t-distributed stochastic neural embedding](https://lvdmaaten.github.io/tsne/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## The VAE framework \n",
    "\n",
    "Let $q_{\\phi}(z|x)$ and $p_{\\theta}(x|z)$ both be parametric distributions. The figure below illustrates the general framework of VAE. \n",
    "\n",
    "<img src=\"figure/vae.png\" alt=\"VAE general framework\" style=\"width: 600px;\"/>\n",
    "<div style=\"text-align:center\"><span style=\"color:purple; font-size:1em;\">Figure 2 - general framework of VAE.</span></div>\n",
    "\n",
    "VAE follows the **encoder-decoder** design. Both the encoder and the decoder can be represented using neural networks that **aim to learn distributions**:\n",
    "\n",
    "1. The **encoder aims to learn the variational approximate posterior $q_{\\phi}(z|x)$. It is a neural network characterized by parameters (weights and biases) $\\phi$. It outputs the parameters for $q_{\\phi}(z|x)$**. For instance, if $q_{\\phi}(z|x)$ is assumed to be Gaussian, the encoder will output the mean and the covariance matrix of the Gaussian. Once the network parameters $\\phi$ are learnt, **it can be used to encode the original sample $x$ to a latent distribution $q_{\\phi}(z|x)$, from which new latent variables $z$ can be sampled**.\n",
    "\n",
    "2. The **decoder aims to learn the likelihood $p_{\\theta}(x|z)$. It is a neural network characterized by the parameters (weights and biases) $\\theta$. It takes a latent variable $z$ sampled from the distribution $q_{\\phi}(z|x)$, and outputs parameters for distribution $p_{\\theta}(x|z)$**. Similar to the encoder, if $p_{\\theta}(x|z)$ is assumed to be Gaussian, the decoder will output the mean and the covariance matrix of the Gaussian. Then we can sample new data point $\\hat{x}$ from $p_{\\theta}(x|z)$, and it will be the **reconstruction of the original input $x$ corresponding to $z$**.\n",
    "\n",
    "The framework of VAE **can be related to the principle component analysis (PCA) in the following way**. In PCA, the latent space is characterized by a $d \\times p$ matrix $\\mathbf{U}$ where the column $\\mathbf{u}_i$ with $0 < i < p$ is the eigenvector of the sample covariance matrix $\\mathbf{X} \\mathbf{X}^{T}$ corresponding to the $i$-th largest eigenvalue. In PCA, encoding a data sample $x$ to $z$ is performed as $z = \\mathbf{U}^{T}x$; and reconstructing the original data sample is performed as $\\hat{x} = \\mathbf{U}\\mathbf{U}^{T}x$. Therefore, the matrix $\\mathbf{U}^{T}$ can be seen as an _encoder_ and $\\mathbf{U}$ can be seen as a _decoder_. VAE can be thought of as the **equivalent of PCA with non-linearities**. **If the non-linear activations are removed from both encoder and decoder neural networks, the VAE might behave similarly to the PCA.**\n",
    "\n",
    "The training process of VAE is done by minimizing the loss function $L(\\theta, \\phi)$ described in (5). We compute its partial derivatives with respect to $\\theta$ and $\\phi$:\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\nabla_{\\theta,\\phi} L(\\theta, \\phi) = - \\nabla_{\\theta,\\phi} \\mathbb{E}_{q_{\\phi}(z|x)} \\left( \\log p_{\\theta}(x|z) \\right) + \\nabla_{\\theta,\\phi} D_{KL} \\left( q_{\\phi}(z|x) \\rVert p_{\\theta}(z) \\right). \\tag{6}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "Let's first look at the second term in equation (6). In many cases, the KL-divergence $D_{KL} \\left( q_{\\phi}(z|x) \\rVert p_{\\theta}(z) \\right)$ in $L(\\theta, \\phi)$ can be derived analytically by considering $q_{\\phi}(z|x)$ and $p_{\\theta}(z)$ to be analytically tractable distributions (see examples below). Therefore, the second term $\\nabla_{\\theta,\\phi} D_{KL} \\left( q_{\\phi}(z|x) \\rVert p_{\\theta}(z) \\right)$ can be computed using standard backpropagation.\n",
    "\n",
    "Now look at the first term $-\\nabla_{\\theta,\\phi} \\mathbb{E}_{q_{\\phi}(z|x)} \\left( \\log p_{\\theta}(x|z) \\right)$ in $L(\\theta, \\phi)$. Notice that under some mild differentiation conditions [1], we can reparametrize the distribution $q_{\\phi}(z|x)$ using an **auxiliary noise random variable $\\epsilon \\sim p(\\epsilon),$ followed by a differentiable deterministic transformation $\\psi$**. That is,\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "z = \\psi \\left( \\epsilon \\right), \\epsilon \\sim p(\\epsilon). \\tag{8}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "To see this, consider two examples:\n",
    "\n",
    "1. Let $q_{\\phi}(z|x)$ be any distribution with continuous, differentiable cumulative distribution function (CDF). Then we can choose $p(\\epsilon)$ to be the Uniform distribution $\\mathcal{U}(0, \\mathbf{I})$ and $\\psi$ be the inverse CDF of $q_{\\phi}(z|x)$ [6].\n",
    "\n",
    "2. Let $q_{\\phi}(z|x)$ be a Gaussian distribution with mean $\\mathbf{\\mu}$, covariance matrix $\\mathbf{\\Sigma}$. Then we can choose $p(\\epsilon)$ to be the standard normal distribution $\\mathcal{N}(0, \\mathbf{I})$ and $\\psi = \\mathbf{\\mu} + \\epsilon\\mathbf{\\Sigma}^{\\frac{1}{2}}$.\n",
    "\n",
    "\n",
    "Between training iterations, the network parameters **$\\theta$ and $\\phi$ stay deterministic**. Therefore, given an auxiliary random variable $\\epsilon$, the deterministic transformation $\\psi$ can be written as a deterministic function $g_{\\phi}$ of input $x$ and $\\epsilon$. That is,\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "z = g_{\\phi} \\left(x, \\epsilon \\right), \\epsilon \\sim p(\\epsilon). \\tag{9}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "Then the following reparametrization can be perfomed:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "- \\nabla_{\\theta,\\phi} \\mathbb{E}_{q_{\\phi}(z|x)} \\left( \\log p_{\\theta}(x|z) \\right) & = - \\nabla_{\\theta,\\phi} \\mathbb{E}_{p(\\epsilon)} \\left( \\log p_{\\theta} \\left(x| g_{\\phi} \\left(x, \\epsilon \\right) \\right) \\right) \\\\\n",
    "& = - \\mathbb{E}_{p(\\epsilon)} \\nabla_{\\theta,\\phi} \\left( \\log p_{\\theta} \\left(x| g_{\\phi} \\left(x, \\epsilon \\right) \\right) \\right),\n",
    "\\end{aligned}\n",
    "\\tag{10}\n",
    "$$\n",
    "\n",
    "where $\\epsilon^{(l)} \\sim p(\\epsilon)$. Notice that, after the reparametrization, the derivative sign and the expectation in equation (10) can be inverted since $p(\\epsilon)$ does not depend on $\\theta$ and $\\phi$. By doing the reparametrization, we are **decoupling the stochastic component related to sampling of $\\epsilon$ and the deterministic component, _i.e._ $\\nabla_{\\theta,\\phi} \\left( \\log p_{\\theta} \\left(x| g_{\\phi} \\left(x, \\epsilon \\right) \\right) \\right)$ given $\\epsilon$**. We can use Mont Carlo sampling to estimate the last quantity in (10):\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "- \\mathbb{E}_{p(\\epsilon)} \\nabla_{\\theta,\\phi} \\left( \\log p_{\\theta} \\left(x| g_{\\phi} \\left(x, \\epsilon \\right) \\right) \\right) \\approx \\frac{1}{L} \\sum_{l=1}^{L} \\nabla_{\\theta,\\phi} \\left( \\log p_{\\theta} \\left(x| g_{\\phi} \\left(x, \\epsilon^{(l)} \\right) \\right) \\right), \\tag{11}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "with $\\epsilon^{(l)} \\sim p(\\epsilon)$.\n",
    "\n",
    "In practice, $p_{\\theta}(x|z^{(l)})$ can often be assumed to be tractable distributions, such as exponential family (for continuous data) or Bernoulli (for binary data). This seems to be a restriction, but in general the network learns to output a distribution with mean very close to the original data $x$ and a variance within reasonable value range. The stochastic components in $p_{\\theta}(x|z^{(l)})$ thus allow for obtaining different reconstructions of $x$ that are slightly variable. Under the exponential or Bernoulli assumptions, the partial derivative estimate in equation (11) can be computed using standard backpropagation (see examples below).\n",
    "\n",
    "### References\n",
    "\n",
    "- [6] [Wikipedia: inverse transform sampling](https://en.wikipedia.org/wiki/Inverse_transform_sampling)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## VAE examples\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Example 1: MNIST dataset\n",
    "\n",
    "MNIST is a dataset for hand-written digits. Here is a visualization of a few sample images from the dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f4895d3ca20>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f4895d3c470>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "import ssl\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def load_all_mnist():\n",
    "    ssl._create_default_https_context = ssl._create_unverified_context\n",
    "    (train, _), (valid, _) = tf.keras.datasets.mnist.load_data()\n",
    "    mnist = np.concatenate((train, valid), axis=0)\n",
    "    return mnist\n",
    "\n",
    "\n",
    "def sample_from(dataset, n_samp):\n",
    "    n_total = dataset.shape[0]\n",
    "    samp_indx = np.random.randint(\n",
    "        low=0, high=n_total, size=n_samp)\n",
    "    return np.take(dataset, indices=samp_indx, axis=0)\n",
    "    \n",
    "\n",
    "n_samples = 10\n",
    "mnist_samples = sample_from(mnist, n_samples)\n",
    "\n",
    "plt.clf()\n",
    "\n",
    "unit = 15\n",
    "figure_size = (unit,n_samples*unit)\n",
    "fig, axes = plt.subplots(1, n_samples, figsize=figure_size)\n",
    "\n",
    "for ax_indx in range(n_samples):\n",
    "    axes[ax_indx].imshow(mnist_samples[ax_indx,:,:], cmap='gray')\n",
    "    axes[ax_indx].set_xticks([])\n",
    "    axes[ax_indx].set_yticks([])\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
